{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72daf175",
   "metadata": {},
   "source": [
    "# Spam Detection using Linear Regression\n",
    "\n",
    "This notebook demonstrates how to build a spam detection system using linear regression.\n",
    "We'll use text feature extraction techniques and logistic regression (which is a linear model)\n",
    "to classify emails as spam or not spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7842328",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9104f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "import kagglehub\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e4a72",
   "metadata": {},
   "source": [
    "## Load Real Spam Email Dataset\n",
    "\n",
    "We'll use a \"real\" spam email dataset from Kaggle to make our analysis more realistic and robust.\n",
    "This dataset contains actual spam and ham (legitimate) emails. The community calls the positive labels ham for I assume reasons of it being a cute little pun (though tbf, I think spam is very delicious so I'm not sure I see where they're coming from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e725af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the spam email dataset from Kaggle\n",
    "print(\"Downloading spam email dataset from Kaggle...\")\n",
    "path = kagglehub.dataset_download(\"jackksoncsie/spam-email-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c48ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import os\n",
    "# List files in the dataset directory\n",
    "dataset_files = os.listdir(path)\n",
    "print(\"Files in dataset:\", dataset_files)\n",
    "\n",
    "# Load the CSV file \n",
    "dataset_file = dataset_files[0]  # Take the first CSV file\n",
    "df = pd.read_csv(os.path.join(path, dataset_file))\n",
    "print(f\"\\nLoaded dataset: {dataset_file}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "print(\"\\nSome random rows:\")\n",
    "print(df.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbba20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for ease of labeling\n",
    "text_col = 'text'\n",
    "label_col = 'spam'\n",
    "df = df.rename(columns={text_col: 'message', label_col: 'label'})\n",
    "\n",
    "# Check unique values in label column\n",
    "print(f\"\\nUnique labels: {df['label'].unique()}\")\n",
    "print(f\"Label counts:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ec357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure labels are integers\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Remove any rows with missing values\n",
    "df = df.dropna(subset=['message', 'label'])\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
    "print(f\"Spam messages: {df[df['label'] == 1].shape[0]}\")\n",
    "print(f\"Ham messages: {df[df['label'] == 0].shape[0]}\")\n",
    "print(f\"Spam ratio: {df['label'].mean():.3f}\")\n",
    "\n",
    "# Show sample messages\n",
    "print(f\"Sample spam messages:\")\n",
    "spam_samples = df[df['label'] == 1]['message'].head(3)\n",
    "for i, msg in enumerate(spam_samples, 1):\n",
    "    print(f\"{i}. {msg[:100]}...\" if len(msg) > 100 else f\"{i}. {msg}\")\n",
    "\n",
    "print(f\"Sample ham messages:\")\n",
    "ham_samples = df[df['label'] == 0]['message'].head(3)\n",
    "for i, msg in enumerate(ham_samples, 1):\n",
    "    print(f\"{i}. {msg[:100]}...\" if len(msg) > 100 else f\"{i}. {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94bcae",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dafe63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few messages\n",
    "print(\"Sample messages:\")\n",
    "print(\"Spam messages:\")\n",
    "print(df[df['label'] == 1]['message'].head(3).values)\n",
    "print(\"Ham messages:\")\n",
    "print(df[df['label'] == 0]['message'].head(3).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe35733",
   "metadata": {},
   "source": [
    "## Understanding Bag of Words (BoW)\n",
    "\n",
    "Before we build our model, let's understand how **Bag of Words** works step by step.\n",
    "Bag of Words is a fundamental text representation technique that converts text into numerical vectors by assigning each word to a one-hot vector and then making the feature into the sum of the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff15ad5",
   "metadata": {},
   "source": [
    "### Step 1: Building the Vocabulary\n",
    "\n",
    "First, let's manually walk through how bag of words is constructed using a few sample messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee08b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample messages for demonstration\n",
    "demo_messages = [\n",
    "    \"Free money now!\",\n",
    "    \"Meeting at noon\",\n",
    "    \"Free lunch offer\",\n",
    "    \"Money back guarantee\"\n",
    "]\n",
    "\n",
    "print(\"Demo messages:\")\n",
    "for i, msg in enumerate(demo_messages):\n",
    "    print(f\"{i+1}. '{msg}'\")\n",
    "\n",
    "# Step 1: Tokenization - split into words\n",
    "print(\"\\nStep 1: Tokenization\")\n",
    "print(\"=\" * 30)\n",
    "tokenized_messages = []\n",
    "for i, msg in enumerate(demo_messages):\n",
    "    tokens = msg.lower().split()  # Simple tokenization\n",
    "    tokenized_messages.append(tokens)\n",
    "    print(f\"Message {i+1}: {tokens}\")\n",
    "\n",
    "# Step 2: Build vocabulary (unique words)\n",
    "print(\"\\nStep 2: Building Vocabulary\")\n",
    "print(\"=\" * 30)\n",
    "vocabulary = set()\n",
    "for tokens in tokenized_messages:\n",
    "    vocabulary.update(tokens)\n",
    "\n",
    "vocabulary = sorted(list(vocabulary))  # Sort for consistent ordering\n",
    "print(f\"Vocabulary: {vocabulary}\")\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "# Step 3: Create word-to-index mapping\n",
    "print(\"\\nStep 3: Word-to-Index Mapping\")\n",
    "print(\"=\" * 30)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "for word, idx in word_to_idx.items():\n",
    "    print(f\"'{word}' -> index {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c6330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert messages to numerical vectors\n",
    "print(\"Step 4: Converting Messages to Vectors\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "bow_matrix = []\n",
    "for i, tokens in enumerate(tokenized_messages):\n",
    "    # Initialize vector with zeros\n",
    "    vector = [0] * len(vocabulary)\n",
    "    \n",
    "    # Count occurrences of each word\n",
    "    for token in tokens:\n",
    "        if token in word_to_idx:\n",
    "            idx = word_to_idx[token]\n",
    "            vector[idx] += 1\n",
    "    \n",
    "    bow_matrix.append(vector)\n",
    "    print(f\"\\nMessage {i+1}: '{demo_messages[i]}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Vector: {vector}\")\n",
    "    \n",
    "    # Show which positions correspond to which words\n",
    "    non_zero_positions = [(idx, count) for idx, count in enumerate(vector) if count > 0]\n",
    "    print(f\"Non-zero positions: {[(vocabulary[idx], count) for idx, count in non_zero_positions]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbeac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Bag of Words matrix\n",
    "import pandas as pd\n",
    "\n",
    "bow_df = pd.DataFrame(bow_matrix, columns=vocabulary)\n",
    "bow_df.index = [f\"Msg {i+1}\" for i in range(len(demo_messages))]\n",
    "\n",
    "print(\"\\nBag of Words Matrix:\")\n",
    "print(\"=\" * 50)\n",
    "print(bow_df)\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(bow_df, annot=True, cmap='Blues', fmt='d', cbar_kws={'label': 'Word Count'})\n",
    "plt.title('Bag of Words Matrix Visualization\\n(Rows = Messages, Columns = Words)')\n",
    "plt.xlabel('Words in Vocabulary')\n",
    "plt.ylabel('Messages')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb50835",
   "metadata": {},
   "source": [
    "### Key Insights from the Manual BoW Construction:\n",
    "\n",
    "1. **Vocabulary Creation**: We collect all unique words across all documents\n",
    "2. **Vector Size**: Each document becomes a vector of size = vocabulary size\n",
    "3. **Word Counts**: Each position in the vector represents the count of that word\n",
    "4. **Sparsity**: Most positions are 0 (words don't appear in most documents)\n",
    "5. **Order Independence**: \"free money\" and \"money free\" will wind up as the same.\n",
    "\n",
    "Now let's use scikit-learn's CountVectorizer to do this automatically for our spam detection task. CountVectorizer is an internal tool for implementing bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22231303",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering\n",
    "\n",
    "We'll use **Bag of Words (CountVectorizer)** to convert text messages\n",
    "into numerical features that our linear regression model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = df['message']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training set spam ratio: {y_train.mean():.2f}\")\n",
    "print(f\"Test set spam ratio: {y_test.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8eeb82",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We'll use a pipeline that combines Bag of Words vectorization with Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f45816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(\n",
    "        max_features=1000,  # Limit to top 1000 features\n",
    "        stop_words='english',  # Remove common English stop words\n",
    "        lowercase=True,  # Convert to lowercase\n",
    "        ngram_range=(1, 1)  # Use both unigrams and bigrams\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3521c0ed",
   "metadata": {},
   "source": [
    "## Examining the Bag of Words Matrix for Our Dataset\n",
    "\n",
    "Let's look at how our actual spam/ham messages are converted to bag of words vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get the fitted vectorizer and examine a random sample\n",
    "bow_vectorizer = pipeline.named_steps['bow']\n",
    "feature_names = bow_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Vocabulary size: {len(feature_names)}\")\n",
    "print(f\"Sample of 20 random words: {random.sample(list(feature_names), 20)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87a31c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Visualize the bag of words matrix for a subset of messages\n",
    "# Select a few messages for visualization\n",
    "sample_indices = [0, 1, 5, 6, 10, 15]  # Mix of spam and ham\n",
    "sample_msgs = [df.iloc[i]['message'] for i in sample_indices]\n",
    "sample_lbls = [df.iloc[i]['label'] for i in sample_indices]\n",
    "\n",
    "# Transform these messages\n",
    "bow_matrix = bow_vectorizer.transform(sample_msgs)\n",
    "\n",
    "# Convert to dense array and create DataFrame\n",
    "bow_dense = bow_matrix.toarray()\n",
    "\n",
    "# Only show features that appear in at least one of these messages\n",
    "feature_mask = bow_dense.sum(axis=0) > 0\n",
    "active_features = feature_names[feature_mask]\n",
    "active_bow_matrix = bow_dense[:, feature_mask]\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "bow_viz_df = pd.DataFrame(\n",
    "    active_bow_matrix,\n",
    "    columns=active_features,\n",
    "    index=[f\"{'SPAM' if lbl else 'HAM'} {i+1}\" for i, lbl in enumerate(sample_lbls)]\n",
    ")\n",
    "\n",
    "print(f\"\\nBag of Words Matrix for {len(sample_msgs)} sample messages:\")\n",
    "print(f\"Showing {len(active_features)} features that appear in these messages\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show the matrix\n",
    "if len(active_features) <= 20:  # If few features, show all\n",
    "    print(bow_viz_df)\n",
    "    \n",
    "    # Visualize as heatmap\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.heatmap(bow_viz_df, annot=True, cmap='Blues', fmt='d', cbar_kws={'label': 'Word Count'})\n",
    "    plt.title('Bag of Words Matrix for Sample Messages\\n(Rows = Messages, Columns = Words)')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Messages')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:  # If many features, show top ones\n",
    "    # Show features with highest variance (most discriminative)\n",
    "    feature_variance = bow_viz_df.var(axis=0)\n",
    "    top_features = feature_variance.nlargest(15).index\n",
    "    \n",
    "    print(bow_viz_df[top_features])\n",
    "    \n",
    "    # Visualize as heatmap\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.heatmap(bow_viz_df[top_features], annot=True, cmap='Blues', fmt='d', cbar_kws={'label': 'Word Count'})\n",
    "    plt.title('Bag of Words Matrix for Sample Messages\\n(Top 15 Most Variable Features)')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Messages')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9c979",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression with Gradient Descent\n",
    "\n",
    "Before we evaluate our scikit-learn model, let's implement logistic regression from scratch\n",
    "using gradient descent to understand what's happening under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cef66",
   "metadata": {},
   "source": [
    "### Mathematical Foundation\n",
    "\n",
    "Logistic regression uses the sigmoid function to map any real number to a probability between 0 and 1:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Where $z = \\mathbf{w}^T \\mathbf{x} + b$ (weights times features plus bias)\n",
    "\n",
    "The cost function (log-loss) we want to minimize is:\n",
    "$$J(\\mathbf{w}) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee1cb4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Implement logistic regression from scratch\n",
    "class LogisticRegressionGD:\n",
    "    \"\"\"Logistic Regression using Gradient Descent\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        # Clip z to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def compute_cost(self, y_true, y_pred):\n",
    "        \"\"\"Compute logistic regression cost (log-loss)\"\"\"\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        m = len(y_true)\n",
    "        cost = -1/m * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return cost\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the logistic regression model using gradient descent\"\"\"\n",
    "        # Initialize parameters\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for i in range(self.max_iterations):\n",
    "            # Forward pass\n",
    "            z = X.dot(self.weights) + self.bias\n",
    "            predictions = self.sigmoid(z)\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self.compute_cost(y, predictions)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1/m) * X.T.dot(predictions - y)\n",
    "            db = (1/m) * np.sum(predictions - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Check for convergence\n",
    "            if i > 0 and abs(self.cost_history[-2] - self.cost_history[-1]) < self.tolerance:\n",
    "                print(f\"Converged after {i+1} iterations\")\n",
    "                break\n",
    "                \n",
    "        print(f\"Final cost: {cost:.6f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        z = X.dot(self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make binary predictions\"\"\"\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb7103",
   "metadata": {},
   "source": [
    "### Training Our Custom Model\n",
    "\n",
    "Let's train our gradient descent implementation on the same bag of words features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bag of words features from our pipeline\n",
    "X_train_bow = pipeline.named_steps['bow'].transform(X_train).toarray()\n",
    "X_test_bow = pipeline.named_steps['bow'].transform(X_test).toarray()\n",
    "\n",
    "print(f\"Training set shape: {X_train_bow.shape}\")\n",
    "print(f\"Test set shape: {X_test_bow.shape}\")\n",
    "\n",
    "# Train our custom logistic regression\n",
    "print(\"Training custom logistic regression with gradient descent...\")\n",
    "custom_lr = LogisticRegressionGD(learning_rate=0.1, max_iterations=1000)\n",
    "custom_lr.fit(X_train_bow, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd48cb0",
   "metadata": {},
   "source": [
    "### Visualizing the Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost function over iterations\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(custom_lr.cost_history)\n",
    "plt.title('Cost Function During Training')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (Log-Loss)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Zoom in on the last part of training\n",
    "plt.subplot(1, 2, 2)\n",
    "start_idx = max(0, len(custom_lr.cost_history) - 200)\n",
    "plt.plot(range(start_idx, len(custom_lr.cost_history)), custom_lr.cost_history[start_idx:])\n",
    "plt.title('Cost Function (Last 200 Iterations)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (Log-Loss)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c0910",
   "metadata": {},
   "source": [
    "### Comparing Our Implementation with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with our custom model\n",
    "custom_y_pred = custom_lr.predict(X_test_bow)\n",
    "custom_y_pred_proba = custom_lr.predict_proba(X_test_bow)\n",
    "\n",
    "# Get scikit-learn predictions for comparison\n",
    "sklearn_y_pred = pipeline.predict(X_test)\n",
    "sklearn_y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compare accuracies\n",
    "custom_accuracy = accuracy_score(y_test, custom_y_pred)\n",
    "sklearn_accuracy = accuracy_score(y_test, sklearn_y_pred)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Custom Gradient Descent Accuracy: {custom_accuracy:.4f}\")\n",
    "print(f\"Scikit-learn Accuracy:           {sklearn_accuracy:.4f}\")\n",
    "print(f\"Difference:                      {abs(custom_accuracy - sklearn_accuracy):.4f}\")\n",
    "\n",
    "# Compare AUC scores\n",
    "custom_auc = roc_auc_score(y_test, custom_y_pred_proba)\n",
    "sklearn_auc = roc_auc_score(y_test, sklearn_y_pred_proba)\n",
    "\n",
    "print(f\"\\nCustom Gradient Descent AUC:     {custom_auc:.4f}\")\n",
    "print(f\"Scikit-learn AUC:                {sklearn_auc:.4f}\")\n",
    "print(f\"Difference:                      {abs(custom_auc - sklearn_auc):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b00f29",
   "metadata": {},
   "source": [
    "### Visualizing Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fb38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# ROC Curves comparison\n",
    "fpr_custom, tpr_custom, _ = roc_curve(y_test, custom_y_pred_proba)\n",
    "fpr_sklearn, tpr_sklearn, _ = roc_curve(y_test, sklearn_y_pred_proba)\n",
    "\n",
    "axes[0, 0].plot(fpr_custom, tpr_custom, color='red', lw=2, \n",
    "                label=f'Custom GD (AUC = {custom_auc:.3f})')\n",
    "axes[0, 0].plot(fpr_sklearn, tpr_sklearn, color='blue', lw=2, \n",
    "                label=f'Scikit-learn (AUC = {sklearn_auc:.3f})')\n",
    "axes[0, 0].plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "axes[0, 0].set_xlim([0.0, 1.0])\n",
    "axes[0, 0].set_ylim([0.0, 1.05])\n",
    "axes[0, 0].set_xlabel('False Positive Rate')\n",
    "axes[0, 0].set_ylabel('True Positive Rate')\n",
    "axes[0, 0].set_title('ROC Curve Comparison')\n",
    "axes[0, 0].legend(loc=\"lower right\")\n",
    "\n",
    "# Prediction probability comparison\n",
    "axes[0, 1].scatter(sklearn_y_pred_proba, custom_y_pred_proba, alpha=0.6)\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Scikit-learn Predicted Probability')\n",
    "axes[0, 1].set_ylabel('Custom GD Predicted Probability')\n",
    "axes[0, 1].set_title('Prediction Probability Comparison')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature weights comparison (top 20 features)\n",
    "feature_names = pipeline.named_steps['bow'].get_feature_names_out()\n",
    "sklearn_coef = pipeline.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# Get top 20 features by absolute weight in sklearn model\n",
    "top_indices = np.argsort(np.abs(sklearn_coef))[-20:]\n",
    "top_features = feature_names[top_indices]\n",
    "sklearn_top_weights = sklearn_coef[top_indices]\n",
    "custom_top_weights = custom_lr.weights[top_indices]\n",
    "\n",
    "x_pos = np.arange(len(top_features))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 0].barh(x_pos - width/2, sklearn_top_weights, width, \n",
    "                label='Scikit-learn', alpha=0.8)\n",
    "axes[1, 0].barh(x_pos + width/2, custom_top_weights, width, \n",
    "                label='Custom GD', alpha=0.8)\n",
    "axes[1, 0].set_yticks(x_pos)\n",
    "axes[1, 0].set_yticklabels(top_features)\n",
    "axes[1, 0].set_xlabel('Weight Value')\n",
    "axes[1, 0].set_title('Top 20 Feature Weights Comparison')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Weight correlation\n",
    "axes[1, 1].scatter(sklearn_coef, custom_lr.weights, alpha=0.6)\n",
    "axes[1, 1].plot([sklearn_coef.min(), sklearn_coef.max()], \n",
    "                [sklearn_coef.min(), sklearn_coef.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('Scikit-learn Weights')\n",
    "axes[1, 1].set_ylabel('Custom GD Weights')\n",
    "axes[1, 1].set_title('All Feature Weights Correlation')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.corrcoef(sklearn_coef, custom_lr.weights)[0, 1]\n",
    "axes[1, 1].text(0.05, 0.95, f'Correlation: {correlation:.4f}', \n",
    "                transform=axes[1, 1].transAxes, fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nWeight correlation between models: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89792a",
   "metadata": {},
   "source": [
    "### Some takeaways\n",
    "\n",
    "1. **Similar Performance**: Our gradient descent implementation achieves very similar results to scikit-learn\n",
    "2. **Learning Process**: We can visualize how the cost function decreases during training\n",
    "3. **Weight Correlation**: The learned weights are highly correlated between implementations. *Why might they be different?*\n",
    "\n",
    "### Why Use Scikit-learn in Practice?\n",
    "\n",
    "It'll do the thing you want quickly. But, you'll sometimes need something customized and then maybe you want to break out the explicit solution. Besides which, if you don't understand what it's doing under the hood things can go very awry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75aa08",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544946dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions (using scikit-learn model)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"AUC Score: {auc_score:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7047e",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a2b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Confusion Matrix')\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "axes[0, 0].set_ylabel('Actual')\n",
    "axes[0, 0].set_xticklabels(['Ham', 'Spam'])\n",
    "axes[0, 0].set_yticklabels(['Ham', 'Spam'])\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.3f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[0, 1].set_xlim([0.0, 1.0])\n",
    "axes[0, 1].set_ylim([0.0, 1.05])\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curve')\n",
    "axes[0, 1].legend(loc=\"lower right\")\n",
    "\n",
    "# Feature Importance (Top Bag of Words features)\n",
    "feature_names = pipeline.named_steps['bow'].get_feature_names_out()\n",
    "coefficients = pipeline.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# Get top 10 positive and negative coefficients\n",
    "top_positive_idx = np.argsort(coefficients)[-10:]\n",
    "top_negative_idx = np.argsort(coefficients)[:10]\n",
    "\n",
    "top_features = np.concatenate([top_negative_idx, top_positive_idx])\n",
    "top_coeffs = coefficients[top_features]\n",
    "top_feature_names = [feature_names[i] for i in top_features]\n",
    "\n",
    "colors = ['red' if coeff < 0 else 'blue' for coeff in top_coeffs]\n",
    "axes[1, 0].barh(range(len(top_coeffs)), top_coeffs, color=colors, alpha=0.7)\n",
    "axes[1, 0].set_yticks(range(len(top_coeffs)))\n",
    "axes[1, 0].set_yticklabels(top_feature_names)\n",
    "axes[1, 0].set_xlabel('Coefficient Value')\n",
    "axes[1, 0].set_title('Top 20 Features (Red=Ham, Blue=Spam)')\n",
    "\n",
    "# Prediction probabilities distribution\n",
    "axes[1, 1].hist(y_pred_proba[y_test == 0], alpha=0.7, label='Ham', bins=10, density=True)\n",
    "axes[1, 1].hist(y_pred_proba[y_test == 1], alpha=0.7, label='Spam', bins=10, density=True)\n",
    "axes[1, 1].set_xlabel('Spam Probability')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title('Distribution of Prediction Probabilities')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34858846",
   "metadata": {},
   "source": [
    "## Testing with New Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a847ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with some new messages\n",
    "test_messages = [\n",
    "    \"Congratulations! You've won a free iPhone! Click here to claim!\",\n",
    "    \"Hey, can we meet for lunch tomorrow?\",\n",
    "    \"URGENT: Your bank account needs verification!\",\n",
    "    \"The meeting is scheduled for 2 PM in conference room A\",\n",
    "    \"Get rich quick with this amazing opportunity!\"\n",
    "]\n",
    "\n",
    "print(\"Testing with new messages:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, message in enumerate(test_messages, 1):\n",
    "    prediction = pipeline.predict([message])[0]\n",
    "    probability = pipeline.predict_proba([message])[0]\n",
    "    \n",
    "    label = \"SPAM\" if prediction == 1 else \"HAM\"\n",
    "    spam_prob = probability[1]\n",
    "    \n",
    "    print(f\"\\nMessage {i}: {message}\")\n",
    "    print(f\"Prediction: {label}\")\n",
    "    print(f\"Spam Probability: {spam_prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d16c0e",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "Let's examine what the model learned by looking at the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_names = pipeline.named_steps['bow'].get_feature_names_out()\n",
    "coefficients = pipeline.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Sort by coefficient value\n",
    "feature_importance = feature_importance.sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top 15 Most Important Features:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Positive coefficients indicate spam-like features\")\n",
    "print(\"Negative coefficients indicate ham-like features\")\n",
    "print()\n",
    "\n",
    "for idx, row in feature_importance.head(15).iterrows():\n",
    "    direction = \"SPAM\" if row['coefficient'] > 0 else \"HAM\"\n",
    "    print(f\"{row['feature']:20} | {row['coefficient']:8.3f} | {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2a989",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "In this notebook, we successfully built a spam detection system using logistic regression. However, can you see why this approach might fail if I were to deploy it and not retrain it frequently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d0bf2",
   "metadata": {},
   "source": [
    "### Why This Model Might Fail in Production Without Retraining\n",
    "\n",
    "Let's demonstrate with some examples of how spammers might evolve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80629c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of evolved spam that would likely bypass our model\n",
    "evolved_spam_examples = [\n",
    "    # Character substitution\n",
    "    \"Fr33 M0n3y N0W! C1ick h3r3 for amaz1ng d3als!\",\n",
    "    \n",
    "    # New technology terms (post-training)\n",
    "    \"Exclusive NFT drop! Mint your crypto fortune today!\",\n",
    "    \n",
    "    # Misspellings and creative spacing\n",
    "    \"F R E E   G I F T S   A V A I L A B L E   N O W\",\n",
    "    \n",
    "    # New scam types\n",
    "    \"Your Amazon Prime account needs verification. Click here to avoid suspension.\",\n",
    "    \n",
    "    # Social media style\n",
    "    \"OMG bestie! This side hustle is literally printing money 💰💰💰 DM me!\"\n",
    "]\n",
    "\n",
    "print(\"Testing our model on evolved spam examples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, message in enumerate(evolved_spam_examples, 1):\n",
    "    prediction = pipeline.predict([message])[0]\n",
    "    probability = pipeline.predict_proba([message])[0]\n",
    "    \n",
    "    label = \"SPAM\" if prediction == 1 else \"HAM\"\n",
    "    spam_prob = probability[1]\n",
    "    \n",
    "    print(f\"\\nExample {i}: {message}\")\n",
    "    print(f\"Prediction: {label}\")\n",
    "    print(f\"Spam Probability: {spam_prob:.3f}\")\n",
    "    \n",
    "    # Check if any words from this message are in our vocabulary\n",
    "    bow_vector = pipeline.named_steps['bow'].transform([message])\n",
    "    recognized_words = bow_vector.nnz\n",
    "    total_words = len(message.split())\n",
    "    \n",
    "    print(f\"Words recognized by model: {recognized_words}/{total_words} ({recognized_words/total_words:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530aa79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analysis complete!\")\n",
    "print(f\"Final model accuracy: {accuracy:.1%}\")\n",
    "print(f\"Final model AUC score: {auc_score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
